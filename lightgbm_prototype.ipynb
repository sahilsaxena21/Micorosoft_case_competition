{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.6.8",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "name": "deploy-to-aci-04",
    "notebookId": 4378841618341025,
    "kernelspec": {
      "display_name": "Python (reco_base)",
      "language": "python",
      "name": "reco_base"
    },
    "authors": [
      {
        "name": "pasha"
      }
    ],
    "colab": {
      "name": "lightgbm_prototype.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Fz_wLU5Rcp",
        "colab_type": "text"
      },
      "source": [
        "## Deploying a Real-Time Content Based Personalization Model\n",
        "\n",
        "This notebook provides an prototype for how Touchpoint can use machine learning to automate content based personalization for their customers by using a recommendation system. Azure Databricks is used to train a model that predicts the probability a user will engage with an item. In turn, this estimate can be used to rank items based on the content that a user is most likely to consume.<br><br>\n",
        "This notebook creates a scalable real-time scoring service for the Spark based models such as the Content Based Personalization model trained in the MMLSpark_LightGBM_Prototype notebook\n",
        "<br><br>\n",
        "### Architecture\n",
        "<img src=\"https://recodatasets.blob.core.windows.net/images/lightgbm_criteo_arch.svg\" alt=\"Architecture\">\n",
        "\n",
        "### Components\n",
        "The following components are used in this architecture:<br>\n",
        "- [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) is a storage service optimized for storing massive amounts of unstructured data. In this case, the input data is stored here.<br>\n",
        "- [Azure Databricks](https://azure.microsoft.com/en-us/services/databricks/) is a managed Apache Spark cluster where model training and evaluating is performed.<br>\n",
        "- [Azure Machine Learning service](https://azure.microsoft.com/en-us/services/machine-learning-service/) is used in this scenario to register the machine learning model. <br>\n",
        "- [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/) is used to package the scoring script as a container image which is used to serve the model in production. <br>\n",
        "- [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/) is used to deploy the trained models to web or app services. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLgd8wm95Rcr",
        "colab_type": "text"
      },
      "source": [
        "## Score Service Steps\n",
        "In this prototype, a \"scoring service\" is a function that is executed by a docker container. It takes in a post request with JSON formatted payload and produces a score based on a previously estimated model. In our case, we will use the model we developed (see mmlspark_lightgbm_prototype). Because that model was trained using PySpark we will create a Spark session on a single instance (within the docker container) which will use [MML Spark Serving](https://github.com/Azure/mmlspark/blob/master/docs/mmlspark-serving.md) to execute the model on the received input data and return the probability of interaction. We will use Azure Machine Learning to create and run the docker container.\n",
        "\n",
        "In order to create a scoring service, we will do the following steps:\n",
        "\n",
        "1. Setup and authorize the Azure Machine Learning Workspace\n",
        "2. Serialize the previously trained model and add it to the Azure Model Registry\n",
        "3. Define the 'scoring service' script to execute the model\n",
        "4. Define all the pre-requisites that that script requires\n",
        "5. Use the model, the driver script, and the pre-requisites to create a Azure Container Image\n",
        "6. Deploy the container image on a scalable platform Azure Kubernetes Service\n",
        "7. Test the service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv5R3oqs5Rcr",
        "colab_type": "text"
      },
      "source": [
        "### Setup libraries and variables\n",
        "\n",
        "The next few cells initialize the environment and variables: we import relevant libraries and set variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmW_9zCH5Rcs",
        "colab_type": "code",
        "colab": {},
        "outputId": "ba7e78f8-3220-4a17-8195-f49a3c0bde91"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "from reco_utils.dataset.criteo import get_spark_schema, load_spark_df\n",
        "from reco_utils.azureml.aks_utils import qps_to_replicas, replicas_to_qps, nodes_to_replicas\n",
        "\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import VERSION as azureml_version\n",
        "\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.conda_dependencies import CondaDependencies \n",
        "from azureml.core.webservice import Webservice, AksWebservice\n",
        "from azureml.core.image import ContainerImage\n",
        "from azureml.core.compute import AksCompute, ComputeTarget\n",
        "\n",
        "from math import floor\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"Azure ML SDK version: {}\".format(azureml_version))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\">Azure ML SDK version: 1.3.0\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCCcmOxk5Rcw",
        "colab_type": "text"
      },
      "source": [
        "## Configure Scoring Service Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il4i_2wu5Rcw",
        "colab_type": "code",
        "colab": {},
        "outputId": "b72081d8-bbc2-44f8-9b5c-efdd330e13f9"
      },
      "source": [
        "MODEL_NAME = 'lightgbm_prototype.mml'  # this name must exactly match the name used to save the pipeline model in the estimation notebook\n",
        "MODEL_DESCRIPTION = 'LightGBM Prototype Model'\n",
        "\n",
        "# Setup AzureML assets (names must be lower case alphanumeric without spaces and between 3 and 32 characters)\n",
        "# Azure ML Webservice\n",
        "SERVICE_NAME = 'lightgbm-prototype'\n",
        "# Azure ML Container Image\n",
        "CONTAINER_NAME = SERVICE_NAME\n",
        "CONTAINER_RUN_TIME = 'spark-PY'\n",
        "# Azure Kubernetes Service (AKS)\n",
        "AKS_NAME = 'predict-aks'\n",
        "\n",
        "# Names of other files that are used below\n",
        "CONDA_FILE = \"deploy_conda.yaml\"\n",
        "DRIVER_FILE = \"mmlspark_serving.py\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\"></div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kwXT8P_5Rcz",
        "colab_type": "text"
      },
      "source": [
        "## Setup AzureML Workspace\n",
        "Workspace configuration can be retrieved from the portal and uploaded to Databricks<br>\n",
        "See [AzureML on Databricks](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment#azure-databricks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsxewtQZ5Rc0",
        "colab_type": "code",
        "colab": {},
        "outputId": "ab5a8755-e099-41d3-e71f-3db205cce3b7"
      },
      "source": [
        "!pip install tqdm\n",
        "!pip install papermill\n",
        "from azureml.core import Workspace\n",
        "\n",
        "subscription_id = '796515a0-d9b7-4ab5-9507-440d24feca8e'\n",
        "resource_group  = 'azure_competition'\n",
        "workspace_name  = 'workspace_ml'\n",
        "\n",
        "try:\n",
        "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "    ws.write_config()\n",
        "    print('Library configuration succeeded')\n",
        "except:\n",
        "    print('Workspace not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\">Library configuration succeeded\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptmxzApL5Rc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csVFn4dg5Rc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2jQ76jq5Rc_",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the Serialized Model\n",
        "\n",
        "In order to create the docker container, the first thing we will do is to prepare the model we estimated in a prior step so that the docker container we are creating will be able to access it. We do this by *registering* the model to the workspace (see the Azure ML [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-model-management-and-deployment) for additional details).\n",
        "\n",
        "The model has been stored as a directory on dbfs, and before we register it, we do a few additional steps to facilitate the process.\n",
        "\n",
        "### Input Schema\n",
        "\n",
        "Spark Serving requires the schema of the raw input data. Therefore, we get the schema and \n",
        "store it as an additional file in the model directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSyz1JOJ5RdC",
        "colab_type": "code",
        "colab": {},
        "outputId": "62b32c28-4c8a-489f-fb23-9f86599b5c96"
      },
      "source": [
        "raw_schema = get_spark_schema()\n",
        "with open(os.path.join('/dbfs', MODEL_NAME, 'schema.json'), 'w') as f:\n",
        "  f.write(raw_schema.json())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\"></div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG8oGR215RdI",
        "colab_type": "text"
      },
      "source": [
        "### Copy the model from dbfs to local\n",
        "\n",
        "While you can access files on DBFS with local file APIs, it is safer to explicitly copy saved models to and from dbfs, because the local file APIs can only access files smaller than 2 GB (see details [here](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#access-dbfs-using-local-file-apis))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RYwj1Bf5RdI",
        "colab_type": "code",
        "colab": {},
        "outputId": "1ca3a6e6-9ff6-4eab-95c0-c89625b10550"
      },
      "source": [
        "model_local = os.path.join(os.getcwd(), MODEL_NAME)\n",
        "dbutils.fs.cp('dbfs:/' + MODEL_NAME, 'file:' + model_local, recurse=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\">Out[8]: True</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ZqJu6s5RdL",
        "colab_type": "text"
      },
      "source": [
        "### Register the Model\n",
        "\n",
        "Now we are ready to register the model in the Azure Machine Learning Workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8SJKiS_5RdM",
        "colab_type": "code",
        "colab": {},
        "outputId": "020d6fbd-a401-491a-8163-066f4804756c"
      },
      "source": [
        "# First the model directory is compressed to minimize data transfer\n",
        "zip_file = shutil.make_archive(base_name=MODEL_NAME, format='zip', root_dir=model_local)\n",
        "\n",
        "# Register the model\n",
        "model = Model.register(model_path=zip_file,  # this points to a local file\n",
        "                       model_name=MODEL_NAME,  # this is the name the model is registered as\n",
        "                       description=MODEL_DESCRIPTION,\n",
        "                       workspace=ws)\n",
        "\n",
        "print(model.name, model.description, model.version)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\">Registering model lightgbm_criteo.mml\n",
              "lightgbm_criteo.mml LightGBM Criteo Model 1\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmQSQMO75RdP",
        "colab_type": "text"
      },
      "source": [
        "## Define the Scoring Script\n",
        "\n",
        "Next, we need to create the driver script that will be executed when the service is called. The functions that need to be defined for scoring are `init()` and `run()`. The `init()` function is run when the service is created, and the `run()` function is run each time the service is called.\n",
        "\n",
        "In our prototype, we use the `init()` function to load all the libraries, initialize the spark session, start the spark streaming service and load the model pipeline. We use the `run()` method to route the input to the spark streaming service to generate predictions (in this case the probability of an interaction) then return the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbsjS8R_5RdQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "2088ebf1-af2e-4f20-fef7-13e5593203ea"
      },
      "source": [
        "driver_file = '''\n",
        "import os\n",
        "import json\n",
        "from time import sleep\n",
        "from uuid import uuid4\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from azureml.core.model import Model\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType\n",
        "import requests\n",
        "\n",
        "\n",
        "def init():\n",
        "    \"\"\"One time initialization of pyspark and model server\"\"\"\n",
        "\n",
        "    spark = SparkSession.builder.appName(\"Model Server\").getOrCreate()\n",
        "    import mmlspark  # this is needed to load mmlspark libraries\n",
        "\n",
        "    # extract and load model\n",
        "    model_path = Model.get_model_path('{model_name}')\n",
        "    with ZipFile(model_path, 'r') as f:\n",
        "        f.extractall('model')\n",
        "    model = PipelineModel.load('model')\n",
        "\n",
        "    # load data schema saved with model\n",
        "    with open(os.path.join('model', 'schema.json'), 'r') as f:\n",
        "        schema = StructType.fromJson(json.load(f))\n",
        "\n",
        "    input_df = (\n",
        "        spark.readStream.continuousServer()\n",
        "        .address(\"localhost\", 8089, \"predict\")\n",
        "        .load()\n",
        "        .parseRequest(schema)\n",
        "    )\n",
        "\n",
        "    output_df = (\n",
        "        model.transform(input_df)\n",
        "        .makeReply(\"probability\")\n",
        "    )\n",
        "\n",
        "    checkpoint = os.path.join('/tmp', 'checkpoints', uuid4().hex)\n",
        "    server = (\n",
        "        output_df.writeStream.continuousServer()\n",
        "        .trigger(continuous=\"30 seconds\")\n",
        "        .replyTo(\"predict\")\n",
        "        .queryName(\"prediction\")\n",
        "        .option(\"checkpointLocation\", checkpoint)\n",
        "        .start()\n",
        "    )\n",
        "\n",
        "    # let the server finish starting\n",
        "    sleep(1)\n",
        "\n",
        "\n",
        "def run(input_json):\n",
        "    try:\n",
        "        response = requests.post(data=input_json, url='http://localhost:8089/predict')\n",
        "        result = response.json()['probability']['values'][1]\n",
        "    except Exception as e:\n",
        "        result = str(e)\n",
        "    \n",
        "    return json.dumps({{\"result\": result}})\n",
        "    \n",
        "'''.format(model_name=MODEL_NAME)\n",
        "\n",
        "# check syntax\n",
        "exec(driver_file)\n",
        "\n",
        "with open(DRIVER_FILE, \"w\") as f:\n",
        "    f.write(driver_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\"></div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oko98mD35RdV",
        "colab_type": "text"
      },
      "source": [
        "## Define Dependencies\n",
        "\n",
        "Next, we define the dependencies that are required by the driver script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT3b4Y7l5RdV",
        "colab_type": "code",
        "colab": {},
        "outputId": "daa583da-a155-4bcb-b0ba-3d7a9b175a42"
      },
      "source": [
        "# azureml-sdk is required to load the registered model\n",
        "conda_file = CondaDependencies.create(pip_packages=['azureml-sdk', 'requests']).serialize_to_string()\n",
        "\n",
        "with open(CONDA_FILE, \"w\") as f:\n",
        "    f.write(conda_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\"></div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQGx9ZRm5Rdb",
        "colab_type": "text"
      },
      "source": [
        "## Create the Image\n",
        "\n",
        "We use the `ContainerImage` class to first configure the image with the defined driver and dependencies, then to create the image for use later.<br>\n",
        "Building the image allows it to be downloaded and debugged locally using docker, see [troubleshooting instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-troubleshoot-deployment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd_8j9-25Rdb",
        "colab_type": "code",
        "colab": {},
        "outputId": "f66b689c-90c4-45ea-afec-add4a831ab60"
      },
      "source": [
        "image_config = ContainerImage.image_configuration(execution_script=DRIVER_FILE, \n",
        "                                                  runtime=CONTAINER_RUN_TIME,\n",
        "                                                  conda_file=CONDA_FILE,\n",
        "                                                  tags={\"runtime\":CONTAINER_RUN_TIME, \"model\": MODEL_NAME})\n",
        "\n",
        "image = ContainerImage.create(name=CONTAINER_NAME,\n",
        "                              models=[model],\n",
        "                              image_config=image_config,\n",
        "                              workspace=ws)\n",
        "\n",
        "image.wait_for_creation(show_output=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\">/local_disk0/tmp/1587494352409-0/PythonShell.py:4: DeprecationWarning: ContainerImage class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n",
              "  import errno\n",
              "/local_disk0/tmp/1587494352409-0/PythonShell.py:9: DeprecationWarning: Image class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n",
              "  import signal\n",
              "Creating image\n",
              "Running..............................................................\n",
              "Succeeded\n",
              "Image creation operation finished for image lightgbm-criteo:1, operation &#34;Succeeded&#34;\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwp_mrn85Rde",
        "colab_type": "text"
      },
      "source": [
        "## Create the Service\n",
        "\n",
        "Once we have created an image, we configure an Azure Kubernetes Service (AKS) and deploy the image as an AKS Webservice.\n",
        "\n",
        "**NOTE** We *can* create a service directly from the registered model and image_configuration with the `Webservice.deploy_from_model()` function. \n",
        " We create the image here explicitly and use `deploy_from_image()` for three reasons:\n",
        "\n",
        "1. It provides more transparency in terms of the actual steps that are taking place\n",
        "2. It provides more flexibility and control. For example, you can create images with names that are independent of the service that you are creating. This can be useful in cases where your images are used across multiple services.\n",
        "3. It has potential for faster iteration and for more portability. Once we have an image, we can create a new deployment with the exact same code.\n",
        "\n",
        "### Setup and Planning\n",
        "\n",
        "When we are setting up a production service, we should start by estimating the load we would like to support. In order to estimate that, we need to estimate how long a single call is likely to take. In this prototype, we have done some local tests, and we have estimated that a single query may take approximately 100 ms to process. \n",
        "\n",
        "Based on a few additional assumptions, we can estimate how many replicas are required to support a targetted number of queries per second (qps). \n",
        "\n",
        "**Note:** This estimate should be used as a ballpark figure to get started, and we can verify performance with subsequent load testing to hone in on better estimates. See this [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#aks) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHaOlcxT5Rdf",
        "colab_type": "text"
      },
      "source": [
        "We have written some helper functions to support this type of calculation, and we will use them to estimate the number of replicas required to support loads of 25, 50, 100, 200, and 350 queries per second, using 100 ms as our estimate of the time to complete a single query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUmLZp1S5Rdf",
        "colab_type": "code",
        "colab": {},
        "outputId": "96321682-7e15-4695-a130-68ddee15795f"
      },
      "source": [
        "all_target_qps = [25, 50, 100, 200, 350]\n",
        "query_processing_time = 0.1  ## in seconds\n",
        "replica_estimates = {t: qps_to_replicas(t, query_processing_time) for t in all_target_qps}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\"></div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T89Pi35W5Rdi",
        "colab_type": "text"
      },
      "source": [
        "Based on the size of our customer base and other considerations (e.g. upcoming announcements that may boost traffic, etc), we make a decision on the maximum load we want to support. In this prototype, we will say we want to support 100 queries per second, and that will indicate that we should use the corresponding number of replicas (15 based on the estimates above). \n",
        "\n",
        "Once we have the number of replicas, we then need to make sure we have enough resources (Cores and Memory) within our Azure Kubernetes Service to support that number of replicas. In order to estimate that number, we need to know how many cores are going to be assigned to each replica. This number can be fractional, because there are many use-cases where there are multiple replicas per core. We will confirm this during the next round using additional Microsoft resources such as this [here](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#cpu-units). When we create the Webservice below, we will allocate 0.3 `cpu_cores` and 0.5 GB of memory to each replica. To support 15 replicas, we need `15*0.3` cores and `15*0.5` GB of memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp-s_M1o5Rdi",
        "colab_type": "code",
        "colab": {},
        "outputId": "3ca648e8-1ddf-4cb9-c19f-3b9ff0080731"
      },
      "source": [
        "cpu_cores_per_replica = 0.3\n",
        "print('{} cores required'.format(replica_estimates[100]*cpu_cores_per_replica))\n",
        "print('{} GB of memory required'.format(replica_estimates[100]*0.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\">4.5 cores required\n",
              "7.5 GB of memory required\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2OulkUn_EVv",
        "colab_type": "text"
      },
      "source": [
        "We were unable to deploy the model because we were unable to secure the required Azure computational resources during the competition period. However, the code for deploying is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iAe5Sa35Rdl",
        "colab_type": "code",
        "colab": {},
        "outputId": "6955209b-8053-486a-ded9-498aaa8f677f"
      },
      "source": [
        "# Create AKS compute first\n",
        "\n",
        "# Use the default configuration (can also provide parameters to customize)\n",
        "prov_config = AksCompute.provisioning_configuration()\n",
        "\n",
        "# Create the cluster\n",
        "aks_target = ComputeTarget.create(\n",
        "  workspace=ws, \n",
        "  name=AKS_NAME, \n",
        "  provisioning_configuration=prov_config\n",
        ")\n",
        "\n",
        "aks_target.wait_for_completion(show_output=True)\n",
        "\n",
        "print(aks_target.provisioning_state)\n",
        "print(aks_target.provisioning_errors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style scoped>\n",
              "  .ansiout {\n",
              "    display: block;\n",
              "    unicode-bidi: embed;\n",
              "    white-space: pre-wrap;\n",
              "    word-wrap: break-word;\n",
              "    word-break: break-all;\n",
              "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
              "    font-size: 13px;\n",
              "    color: #555;\n",
              "    margin-left: 4px;\n",
              "    line-height: 19px;\n",
              "  }\n",
              "</style>\n",
              "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
              "<span class=\"ansi-red-fg\">HTTPError</span>                                 Traceback (most recent call last)\n",
              "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/azureml/core/compute/compute.py</span> in <span class=\"ansi-cyan-fg\">_create_compute_target</span><span class=\"ansi-blue-fg\">(workspace, name, compute_payload, target_class)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
              "<span class=\"ansi-green-fg\">--&gt; 335</span><span class=\"ansi-red-fg\">             </span>resp<span class=\"ansi-blue-fg\">.</span>raise_for_status<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         <span class=\"ansi-green-fg\">except</span> requests<span class=\"ansi-blue-fg\">.</span>exceptions<span class=\"ansi-blue-fg\">.</span>HTTPError<span class=\"ansi-blue-fg\">:</span>\n",
              "\n",
              "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/requests/models.py</span> in <span class=\"ansi-cyan-fg\">raise_for_status</span><span class=\"ansi-blue-fg\">(self)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    940</span>         <span class=\"ansi-green-fg\">if</span> http_error_msg<span class=\"ansi-blue-fg\">:</span>\n",
              "<span class=\"ansi-green-fg\">--&gt; 941</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">raise</span> HTTPError<span class=\"ansi-blue-fg\">(</span>http_error_msg<span class=\"ansi-blue-fg\">,</span> response<span class=\"ansi-blue-fg\">=</span>self<span class=\"ansi-blue-fg\">)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    942</span> \n",
              "\n",
              "<span class=\"ansi-red-fg\">HTTPError</span>: 400 Client Error: Bad Request for url: https://management.azure.com//subscriptions/796515a0-d9b7-4ab5-9507-440d24feca8e/resourceGroups/azure_competition/providers/Microsoft.MachineLearningServices/workspaces/workspace_ml/computes/predict-aks?api-version=2019-11-01\n",
              "\n",
              "During handling of the above exception, another exception occurred:\n",
              "\n",
              "<span class=\"ansi-red-fg\">ComputeTargetException</span>                    Traceback (most recent call last)\n",
              "<span class=\"ansi-green-fg\">&lt;command-4378841618341054&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>   workspace<span class=\"ansi-blue-fg\">=</span>ws<span class=\"ansi-blue-fg\">,</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">      9</span>   name<span class=\"ansi-blue-fg\">=</span>AKS_NAME<span class=\"ansi-blue-fg\">,</span>\n",
              "<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\">   </span>provisioning_configuration<span class=\"ansi-blue-fg\">=</span>prov_config\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> )\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> \n",
              "\n",
              "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/azureml/core/compute/compute.py</span> in <span class=\"ansi-cyan-fg\">create</span><span class=\"ansi-blue-fg\">(workspace, name, provisioning_configuration)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    307</span>                                      &#34; {} is a reserved name.&#34;.format(name))\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    308</span>         compute_type <span class=\"ansi-blue-fg\">=</span> provisioning_configuration<span class=\"ansi-blue-fg\">.</span>_compute_type\n",
              "<span class=\"ansi-green-fg\">--&gt; 309</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> compute_type<span class=\"ansi-blue-fg\">.</span>_create<span class=\"ansi-blue-fg\">(</span>workspace<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">,</span> provisioning_configuration<span class=\"ansi-blue-fg\">)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    310</span> \n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    311</span>     <span class=\"ansi-blue-fg\">@</span>staticmethod\n",
              "\n",
              "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/azureml/core/compute/aks.py</span> in <span class=\"ansi-cyan-fg\">_create</span><span class=\"ansi-blue-fg\">(workspace, name, provisioning_configuration)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    137</span>         compute_create_payload = AksCompute._build_create_payload(provisioning_configuration,\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    138</span>                                                                   workspace.location, workspace.subscription_id)\n",
              "<span class=\"ansi-green-fg\">--&gt; 139</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> ComputeTarget<span class=\"ansi-blue-fg\">.</span>_create_compute_target<span class=\"ansi-blue-fg\">(</span>workspace<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">,</span> compute_create_payload<span class=\"ansi-blue-fg\">,</span> AksCompute<span class=\"ansi-blue-fg\">)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    140</span> \n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    141</span>     <span class=\"ansi-blue-fg\">@</span>staticmethod\n",
              "\n",
              "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/azureml/core/compute/compute.py</span> in <span class=\"ansi-cyan-fg\">_create_compute_target</span><span class=\"ansi-blue-fg\">(workspace, name, compute_payload, target_class)</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    338</span>                                          <span class=\"ansi-blue-fg\">&#39;Response Code: {}\\n&#39;</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    339</span>                                          <span class=\"ansi-blue-fg\">&#39;Headers: {}\\n&#39;</span>\n",
              "<span class=\"ansi-green-fg\">--&gt; 340</span><span class=\"ansi-red-fg\">                                          &#39;Content: {}&#39;.format(resp.status_code, resp.headers, resp.content))\n",
              "</span><span class=\"ansi-green-intense-fg ansi-bold\">    341</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">&#39;Azure-AsyncOperation&#39;</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> resp<span class=\"ansi-blue-fg\">.</span>headers<span class=\"ansi-blue-fg\">:</span>\n",
              "<span class=\"ansi-green-intense-fg ansi-bold\">    342</span>             raise ComputeTargetException(&#39;Error, missing operation location from resp headers:\\n&#39;\n",
              "\n",
              "<span class=\"ansi-red-fg\">ComputeTargetException</span>: ComputeTargetException:\n",
              "\tMessage: Received bad response from Resource Provider:\n",
              "Response Code: 400\n",
              "Headers: {&#39;Cache-Control&#39;: &#39;no-cache&#39;, &#39;Pragma&#39;: &#39;no-cache&#39;, &#39;Content-Length&#39;: &#39;314&#39;, &#39;Content-Type&#39;: &#39;application/json; charset=utf-8&#39;, &#39;Expires&#39;: &#39;-1&#39;, &#39;x-ms-correlation-request-id&#39;: &#39;ce739010-8509-4565-ba41-dcad27b260ba&#39;, &#39;x-ms-ratelimit-remaining-subscription-writes&#39;: &#39;1199&#39;, &#39;Request-Context&#39;: &#39;appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d&#39;, &#39;x-ms-request-id&#39;: &#39;|d6ab8eaaa3d54842aea5ab82976ba4f5.87a48563a9fa3c4d.3a93f888_&#39;, &#39;Strict-Transport-Security&#39;: &#39;max-age=31536000; includeSubDomains&#39;, &#39;x-ms-routing-request-id&#39;: &#39;WESTUS2:20200421T202407Z:ce739010-8509-4565-ba41-dcad27b260ba&#39;, &#39;X-Content-Type-Options&#39;: &#39;nosniff&#39;, &#39;Date&#39;: &#39;Tue, 21 Apr 2020 20:24:07 GMT&#39;}\n",
              "Content: b&#39;{&#34;error&#34;:{&#34;code&#34;:&#34;BadArgument&#34;,&#34;message&#34;:&#34;A compute with the same name already exists. Updating property: provisioningState for compute is not supported yet.&#34;,&#34;innererror&#34;:{&#34;clientRequestId&#34;:&#34;1d40eb2f-154b-4013-9c6a-6e8c6273b575&#34;,&#34;serviceRequestId&#34;:&#34;|d6ab8eaaa3d54842aea5ab82976ba4f5.87a48563a9fa3c4d.3a93f888_&#34;}}}&#39;\n",
              "\tInnerException None\n",
              "\tErrorResponse \n",
              "{\n",
              "    &#34;error&#34;: {\n",
              "        &#34;message&#34;: &#34;Received bad response from Resource Provider:\\nResponse Code: 400\\nHeaders: {&#39;Cache-Control&#39;: &#39;no-cache&#39;, &#39;Pragma&#39;: &#39;no-cache&#39;, &#39;Content-Length&#39;: &#39;314&#39;, &#39;Content-Type&#39;: &#39;application/json; charset=utf-8&#39;, &#39;Expires&#39;: &#39;-1&#39;, &#39;x-ms-correlation-request-id&#39;: &#39;ce739010-8509-4565-ba41-dcad27b260ba&#39;, &#39;x-ms-ratelimit-remaining-subscription-writes&#39;: &#39;1199&#39;, &#39;Request-Context&#39;: &#39;appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d&#39;, &#39;x-ms-request-id&#39;: &#39;|d6ab8eaaa3d54842aea5ab82976ba4f5.87a48563a9fa3c4d.3a93f888_&#39;, &#39;Strict-Transport-Security&#39;: &#39;max-age=31536000; includeSubDomains&#39;, &#39;x-ms-routing-request-id&#39;: &#39;WESTUS2:20200421T202407Z:ce739010-8509-4565-ba41-dcad27b260ba&#39;, &#39;X-Content-Type-Options&#39;: &#39;nosniff&#39;, &#39;Date&#39;: &#39;Tue, 21 Apr 2020 20:24:07 GMT&#39;}\\nContent: b&#39;{\\&#34;error\\&#34;:{\\&#34;code\\&#34;:\\&#34;BadArgument\\&#34;,\\&#34;message\\&#34;:\\&#34;A compute with the same name already exists. Updating property: provisioningState for compute is not supported yet.\\&#34;,\\&#34;innererror\\&#34;:{\\&#34;clientRequestId\\&#34;:\\&#34;1d40eb2f-154b-4013-9c6a-6e8c6273b575\\&#34;,\\&#34;serviceRequestId\\&#34;:\\&#34;|d6ab8eaaa3d54842aea5ab82976ba4f5.87a48563a9fa3c4d.3a93f888_\\&#34;}}}&#39;&#34;\n",
              "    }\n",
              "}</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPpmlyiA5Rdq",
        "colab_type": "text"
      },
      "source": [
        "### Consideration\n",
        "\n",
        "Because our estimated load requirements are less than the minimums set by Azure Machine Learning, we should consider an alternate approach to estimating the number of replicas to use for the web service. If this is the only service that will run on the AKS cluster, then we are potentially wasting resources by not leveraging all of the compute resources. Initially, we used the expected load to estimate the number of replicas that should be used. Instead of that approach, we can also use the  number of cores in our cluster to estimate the maximum number of replicas that could be supported.\n",
        "\n",
        "In order to estimate the maximum number of replicas, we do need to consider that there is some overhead on each node for the base kubernetes operations as well as the node's operating system and core functionality. We assume 10\\% overhead in this case.\n",
        "\n",
        "**Note** we are using cores in this prototype, but we could also leverage memory requirements instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b30HpO7v5Rdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_replicas_12_cores = nodes_to_replicas(\n",
        "    n_cores_per_node=4, n_nodes=3, cpu_cores_per_replica=cpu_cores_per_replica\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG5Sq02C5Rdt",
        "colab_type": "text"
      },
      "source": [
        "Once we have the number of replicas our cluster will support, we can then estimate the queries per second we believe the AKS cluster could support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsHbCmMI5Rdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replicas_to_qps(max_replicas_12_cores, query_processing_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aXuAk-s5Rdx",
        "colab_type": "text"
      },
      "source": [
        "### Create the Webservice\n",
        "\n",
        "Next, we will configure and create the webservice. In this configuration, we will say each replica will set `cpu_cores=cpu_cores_per_replica` (default `cpu_cores=0.1`). We are adjusting this value based on experience and prior testing with this service. \n",
        "\n",
        "If no arguments are passed to `AksWebservice.deploy_configuration()`, it uses `autoscale_enabled=True` with `autoscale_min_replicas=1` and `autoscale_max_replicas=10`. The max value does not meet our minimum requirements to support 100 queries per second, so we need to adjust it. We can adjust this value to either our estimate based on load (15) or our estimate based on the number that can be supported by the AKS cluster (36). In this prototype, we will set it to the value based on load to allow the AKS cluster to be used for other tasks or services."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MdImiJj5Rdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "webservice_config = AksWebservice.deploy_configuration(cpu_cores=cpu_cores_per_replica,\n",
        "                                                       autoscale_enabled=True,\n",
        "                                                       autoscale_max_replicas=replica_estimates[100])\n",
        "\n",
        "# Deploy service using created image\n",
        "aks_service = Webservice.deploy_from_image(\n",
        "  workspace=ws, \n",
        "  name=SERVICE_NAME,\n",
        "  deployment_config=webservice_config,\n",
        "  image=image,\n",
        "  deployment_target=aks_target\n",
        ")\n",
        "\n",
        "aks_service.wait_for_deployment(show_output=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmosnGnl5Rd0",
        "colab_type": "text"
      },
      "source": [
        "## Test the Service\n",
        "\n",
        "Next, we can use data from the `sample` data to test the service.\n",
        "\n",
        "The service expects JSON as its payload, so we take the sample data, convert to a dictionary, then submit to the service endpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6FjRncX5Rd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# View the URI\n",
        "url = aks_service.scoring_uri\n",
        "print('AKS URI: {}'.format(url))\n",
        "\n",
        "# Setup authentication using one of the keys from aks_service\n",
        "headers = dict(Authorization='Bearer {}'.format(aks_service.get_keys()[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ86JXiE5Rd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab some sample data\n",
        "df = load_spark_df(size='sample', spark=spark, dbutils=dbutils)\n",
        "data = df.head().asDict()\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as5FLmHB5Rd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Send a request to the AKS cluster\n",
        "response = requests.post(url=url, json=data, headers=headers)\n",
        "print(response.json())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSXl9-0G5Rd9",
        "colab_type": "text"
      },
      "source": [
        "### Delete the Service\n",
        "\n",
        "When you are done, you can delete the service to minimize costs. You can always redeploy from the image using the same command above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEO8x2Tn5Rd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment the following line to delete the web service\n",
        "# aks_service.delete()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZWrGXv15ReE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aks_service.state"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}